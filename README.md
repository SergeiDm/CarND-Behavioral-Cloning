# CarND-Behavioral-Cloning
## Project Description
The project consists of learning network to map images, taken from a front-facing camera with steering angles. According to NVIDEA [«End to End Learning for Self-Driving Cars»](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) this approach for self-driving cars may lead to better performance in comparison explicit decomposition of the self-driving problem.

## Project files
The project includes the following folder/files:
- model.py – the script to create and train the model.
- drive.py – for driving the car in autonomous mode.
- model.h5 – the model weights.
- model.json – the model architecture.
- illustrations - the folder with pictures 'for README.md'.
For testing autonomous driving the following code should be executed:
`python drive.py model.json`

## Project Data
Input data is a set of RGB images 160 (height) x 320 (width) pixels, generated by the driving simulator in training mode. 

Examples:

![Image examples](https://github.com/SergeiDm/CarND-Behavioral-Cloning/blob/master/illustrations/image_examples.jpg)

The total number of images used in this project is 14 821. ~30% of them is recovery data.

Output data is steering angles in range [-1, 1].

## Project Pipeline
### Collecting data
All input and output data were recorded by simulator in training mode.
### Preprocessing data
Output data is in range [-1, 1], so there is no need for transformation like normalization.

For input data, the following steps were applied:
- Cropping images for getting value information. An image consists of a lot of irrelevant details (e.g. sky, trees, lake), which prevent network to produce value results. Important things here are lane lines and driving surface. From initial images, 60 pixels from the top and 20 pixels from the bottom were removed, so new image size is 80 x 320 pixels.
- Color converting. There are few places in the road where it’s complicated to distinguish road surface and landscape. It leads the model to produce incorrect results. In order to make elements more distinguishable HSV color space was applied to the images.

![Preprocessing data](https://github.com/SergeiDm/CarND-Behavioral-Cloning/blob/master/illustrations/preprocessing_data.jpg)

In Fig. 4 the right border is more recognizable.
- Normalizing input data by min-max transformation from range [0, 255] to [0, 1]. This technique allows keeping weights small.

### Training and validating model
Data generated by simulator was used as input for convolutional neural network (see CNN architecture and Training strategy).

### Testing model
Model was tested on track 1 of the simulator. The main condition is the vehicle should be in the road and no tire leaves drivable part.

## CNN architecture
Architecture of network depends on input and output data (count, quality, how they were preprocessed). Initially, I used the architecture proposed by NVIDEA for self-driving cars problem, but there were two places where the vehicle tires intersected lane lines, so some changes were applied. 

The final structure of the model is:



